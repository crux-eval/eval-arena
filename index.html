<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />   <!--It is necessary to use the UTF-8 encoding with plotly graphics to get e.g. negative signs to render correctly -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="stylesheet"
  href="https://crux-eval.github.io/static/css/bulma.min.css"
>
<style>
  table { 
    white-space: nowrap;
    text-align: right;
  }
</style>
</head>

<body>

<section class="section">
<div class="container is-max-desktop">
<div class="columns is-centered has-text-centered">
<div class="column is-full">
    <h2 class="title is-3"> Eval-Arena: noise and errors by leaderboard showdowns </h2>
    <h3 class="title is-4"> <a href="https://github.com/crux-eval/eval-arena">Doc/Code</a> </h3>
    <div class="content has-text-justified" style="font-size: 120%;">
    <p>
      We ran pairwise match-ups on thousands of model pairs on various LLM benchmarks.
      The main results and code is <a href="https://github.com/crux-eval/eval-arena">here</a>.
      <ul>
        <li><b>size</b>: number of examples in the benchmark</li>
        <li><b>p5_min</b>: the minimum difference with p-value &lt; 0.05</li>
        <li><b>p5_max</b>: the maximum difference with p-value &gt; 0.05</li>
        <li><b>no_solve</b>: examples not solved by any models</li>
        <li><b>tau-</b>: examples negatively correlated with the overall model quality as measured by Kendall's tau.</li>
        <li><b>sig_noise</b>: median of <a href="signal_noise.html">signal-to-noise ratios</a> for doubling the model size.</li>
        <li><b>details</b>: link to details for each benchmark, aggregating by the models or by examples in each benchmark, and a plot of all models and examples sorted by difficulty. </li>
      </ul>
    </p>

    <p><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>benchmark_id</th>
      <th>size</th>
      <th>p5_min</th>
      <th>p5_max</th>
      <th>no_solve</th>
      <th>tau-</th>
      <th>sig_noise</th>
      <th>link to details</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CRUXEval-input</td>
      <td>800</td>
      <td>3.0%</td>
      <td>3.9%</td>
      <td>2.4%</td>
      <td>9.2%</td>
      <td>1.93</td>
      <td>by <a href="model_CRUXEval-input.html">models </a> |<a href="ex_CRUXEval-input.html"> examples </a>|<a href="ex_v_model_CRUXEval-input.html"> data </a></td>
    </tr>
    <tr>
      <td>CRUXEval-output</td>
      <td>800</td>
      <td>2.6%</td>
      <td>3.1%</td>
      <td>3.0%</td>
      <td>4.5%</td>
      <td>3.21</td>
      <td>by <a href="model_CRUXEval-output.html">models </a> |<a href="ex_CRUXEval-output.html"> examples </a>|<a href="ex_v_model_CRUXEval-output.html"> data </a></td>
    </tr>
    <tr>
      <td>DS1000</td>
      <td>1000</td>
      <td>1.5%</td>
      <td>3.4%</td>
      <td>11.2%</td>
      <td>1.2%</td>
      <td>3.00</td>
      <td>by <a href="model_DS1000.html">models </a> |<a href="ex_DS1000.html"> examples </a>|<a href="ex_v_model_DS1000.html"> data </a></td>
    </tr>
    <tr>
      <td>agi_english</td>
      <td>2546</td>
      <td>2.0%</td>
      <td>2.3%</td>
      <td>1.0%</td>
      <td>20.5%</td>
      <td>5.17</td>
      <td>by <a href="model_agi_english.html">models </a> |<a href="ex_agi_english.html"> examples </a>|<a href="ex_v_model_agi_english.html"> data </a></td>
    </tr>
    <tr>
      <td>arc_challenge</td>
      <td>1165</td>
      <td>2.3%</td>
      <td>2.6%</td>
      <td>14.2%</td>
      <td>11.2%</td>
      <td>2.77</td>
      <td>by <a href="model_arc_challenge.html">models </a> |<a href="ex_arc_challenge.html"> examples </a>|<a href="ex_v_model_arc_challenge.html"> data </a></td>
    </tr>
    <tr>
      <td>gsm8k</td>
      <td>1319</td>
      <td>1.5%</td>
      <td>2.6%</td>
      <td>2.1%</td>
      <td>0.5%</td>
      <td>7.09</td>
      <td>by <a href="model_gsm8k.html">models </a> |<a href="ex_gsm8k.html"> examples </a>|<a href="ex_v_model_gsm8k.html"> data </a></td>
    </tr>
    <tr>
      <td>hellaswag</td>
      <td>10042</td>
      <td>0.4%</td>
      <td>0.5%</td>
      <td>6.1%</td>
      <td>1.6%</td>
      <td>6.41</td>
      <td>by <a href="model_hellaswag.html">models </a> |<a href="ex_hellaswag.html"> examples </a>|<a href="ex_v_model_hellaswag.html"> data </a></td>
    </tr>
    <tr>
      <td>humaneval</td>
      <td>164</td>
      <td>4.9%</td>
      <td>9.8%</td>
      <td>3.7%</td>
      <td>1.2%</td>
      <td>1.12</td>
      <td>by <a href="model_humaneval.html">models </a> |<a href="ex_humaneval.html"> examples </a>|<a href="ex_v_model_humaneval.html"> data </a></td>
    </tr>
    <tr>
      <td>humaneval+</td>
      <td>164</td>
      <td>6.7%</td>
      <td>9.8%</td>
      <td>4.3%</td>
      <td>1.8%</td>
      <td>0.50</td>
      <td>by <a href="model_humaneval+.html">models </a> |<a href="ex_humaneval+.html"> examples </a>|<a href="ex_v_model_humaneval+.html"> data </a></td>
    </tr>
    <tr>
      <td>lcb_codegen</td>
      <td>400</td>
      <td>2.5%</td>
      <td>4.8%</td>
      <td>24.2%</td>
      <td>1.5%</td>
      <td>1.67</td>
      <td>by <a href="model_lcb_codegen.html">models </a> |<a href="ex_lcb_codegen.html"> examples </a>|<a href="ex_v_model_lcb_codegen.html"> data </a></td>
    </tr>
    <tr>
      <td>mbpp</td>
      <td>378</td>
      <td>3.7%</td>
      <td>5.8%</td>
      <td>2.4%</td>
      <td>4.0%</td>
      <td>1.98</td>
      <td>by <a href="model_mbpp.html">models </a> |<a href="ex_mbpp.html"> examples </a>|<a href="ex_v_model_mbpp.html"> data </a></td>
    </tr>
    <tr>
      <td>mbpp+</td>
      <td>378</td>
      <td>4.2%</td>
      <td>5.6%</td>
      <td>9.5%</td>
      <td>5.8%</td>
      <td>1.66</td>
      <td>by <a href="model_mbpp+.html">models </a> |<a href="ex_mbpp+.html"> examples </a>|<a href="ex_v_model_mbpp+.html"> data </a></td>
    </tr>
    <tr>
      <td>mmlu</td>
      <td>14042</td>
      <td>0.9%</td>
      <td>0.8%</td>
      <td>0.6%</td>
      <td>12.4%</td>
      <td>13.24</td>
      <td>by <a href="model_mmlu.html">models </a> |<a href="ex_mmlu.html"> examples </a>|<a href="ex_v_model_mmlu.html"> data </a></td>
    </tr>
    <tr>
      <td>nq</td>
      <td>3610</td>
      <td>1.1%</td>
      <td>1.3%</td>
      <td>31.5%</td>
      <td>5.5%</td>
      <td>5.98</td>
      <td>by <a href="model_nq.html">models </a> |<a href="ex_nq.html"> examples </a>|<a href="ex_v_model_nq.html"> data </a></td>
    </tr>
    <tr>
      <td>piqa</td>
      <td>1838</td>
      <td>1.3%</td>
      <td>1.5%</td>
      <td>5.3%</td>
      <td>8.4%</td>
      <td>1.76</td>
      <td>by <a href="model_piqa.html">models </a> |<a href="ex_piqa.html"> examples </a>|<a href="ex_v_model_piqa.html"> data </a></td>
    </tr>
    <tr>
      <td>siqa</td>
      <td>1954</td>
      <td>1.5%</td>
      <td>2.1%</td>
      <td>14.5%</td>
      <td>19.0%</td>
      <td>0.93</td>
      <td>by <a href="model_siqa.html">models </a> |<a href="ex_siqa.html"> examples </a>|<a href="ex_v_model_siqa.html"> data </a></td>
    </tr>
    <tr>
      <td>tqa</td>
      <td>11313</td>
      <td>0.5%</td>
      <td>0.7%</td>
      <td>11.8%</td>
      <td>2.9%</td>
      <td>12.59</td>
      <td>by <a href="model_tqa.html">models </a> |<a href="ex_tqa.html"> examples </a>|<a href="ex_v_model_tqa.html"> data </a></td>
    </tr>
  </tbody>
</table></p>
    <p>
    Code datasets:
    <ul>
      <li><a href="https://evalplus.github.io/">EvalPlus for MBPP/+, HumanEval+</a> </li>
      <li><a href="https://github.com/openai/human-eval">HumanEval</a> </li>
      <li><a href="https://github.com/google-research/google-research/tree/master/mbpp">MBPP (we used the EvalPlus version instead of original)</a> </li>
      <li><a href="https://livecodebench.github.io/leaderboard.html">LiveCodeBench</a></li>
      <li><a href="https://crux-eval.github.io/">CRUXEval</a></li>
      <li><a href="https://ds1000-code-gen.github.io/">DS1000</a></li>
    </ul>
    </p>
</div>
</div>
</div>
</section>


</body>
</html>