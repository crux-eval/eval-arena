{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data formatting scripts\n",
    "\n",
    "from collections import defaultdict\n",
    "import json, math, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cruxeval(type):\n",
    "    records = []\n",
    "    for fname in glob.glob(f\"evaluation_results/*_temp0.2_{type}.json\"):\n",
    "        name = fname.split('/')[1]\n",
    "        model, temp, typejsonl = name.split('_')\n",
    "        print(model, temp, type)\n",
    "\n",
    "        with open(fname) as f:\n",
    "            res = json.load(f)['raw_scored_generations']\n",
    "            for exid in res:\n",
    "                gotid = np.mean(res[exid])\n",
    "                actualid = exid.replace('sample_', '')\n",
    "                records.append({\n",
    "                    'benchmark_id': f'CRUXEval-{type}',\n",
    "                    'model': model,\n",
    "                    'example_id': f\"CRUXEval-{type}/{actualid}\",\n",
    "                    'pass1': gotid,\n",
    "                    'hyperparams': 'temp0.2'\n",
    "                })\n",
    "    df = pd.DataFrame(records)\n",
    "    display(df.describe())\n",
    "    return df\n",
    "        \n",
    "with open('data/cruxeval_input.jsonl', 'w') as f:\n",
    "    dfi = get_cruxeval('input')\n",
    "    f.write(dfi.to_json(orient='records', lines=True))\n",
    "\n",
    "with open('data/cruxeval_output.jsonl', 'w') as f:\n",
    "    dfo = get_cruxeval('output')\n",
    "    f.write(dfo.to_json(orient='records', lines=True))\n",
    "\n",
    "pass1 = pd.concat([dfi, dfo])\n",
    "display(pass1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate evalplus links\n",
    "\n",
    "import sys\n",
    "from jinja2 import Template\n",
    "\n",
    "sys.path.append('/private/home/sida/git/CodeGen/scripts/sida/arena/data/evalplus')\n",
    "\n",
    "from evalplus.data import get_mbpp_plus, get_human_eval_plus, write_jsonl\n",
    "# has these fields dict_keys(['task_id', 'prompt', 'entry_point', 'canonical_solution', 'test', 'contract', 'base_input', 'atol', 'plus_input'])\n",
    "\n",
    "j2_template = Template(\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\" />   <!--It is necessary to use the UTF-8 encoding with plotly graphics to get e.g. negative signs to render correctly -->\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "<link\n",
    "  rel=\"stylesheet\"\n",
    "  href=\"https://crux-eval.github.io/static/css/bulma.min.css\"\n",
    ">\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "<section class=\"section\">                       \n",
    "<h1> {{ id }} </h1>\n",
    "<pre><code> {{ problem }} </code></pre>\n",
    "<h2>solution</h2>\n",
    "<pre><code> {{ canonical_solution }} </code></pre>\n",
    "<h2>base input</h2>\n",
    "<pre><code> {{ base_input }} </code></pre>\n",
    "<h2>plus input</h2>\n",
    "<pre><code> {{ plus_input }} </code></pre>\n",
    "<br>\n",
    "<h1>Generations</h1>\n",
    "{% for key,value in outputs.iterrows() %}\n",
    "      <p> {{ value['model'] }}. fail_base: {{ value['fail_base'] }}, fail_plus: {{ value['fail_plus'] }} </p>\n",
    "      <pre><code>  {{ value['raw_genereation'] }}</code></pre>\n",
    "{% endfor %}\n",
    "                       \n",
    "</section>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\")\n",
    "\n",
    "def write_benchmark(benchmark_id):\n",
    "    if benchmark_id == 'humaneval+':\n",
    "        get_bench = get_human_eval_plus\n",
    "    else:\n",
    "        get_bench = get_mbpp_plus\n",
    "\n",
    "    samples = [\n",
    "        dict(example_id=task_id,\n",
    "            problem=problem['prompt'],\n",
    "            canonical_solution=problem['canonical_solution'],\n",
    "            base_input=problem['base_input'],\n",
    "            plus_input=problem['plus_input'],\n",
    "        )\n",
    "        for task_id, problem in get_bench().items()\n",
    "    ]\n",
    "    df_prob = pd.DataFrame(samples)\n",
    "\n",
    "    records = []\n",
    "    for fname in glob.glob(f\"data/noise_analysis/*/*.jsonl\"):\n",
    "        with open(fname, 'rt') as f:\n",
    "            records.extend([json.loads(l) for l in f.readlines()])\n",
    "\n",
    "    eval_results = pd.DataFrame(records)\n",
    "    display(eval_results.describe())\n",
    "\n",
    "    benchmark_results = eval_results[eval_results['benchmark_id'] == benchmark_id]\n",
    "\n",
    "    df = df_prob.merge(benchmark_results, on='example_id')\n",
    "\n",
    "    ids = list(set(df['example_id']))\n",
    "    for id in ids:\n",
    "        current_prob = df[df['example_id'] == id]\n",
    "        first = current_prob.iloc[0]\n",
    "        # display(current_prob)\n",
    "        generation = current_prob[['model', 'raw_genereation', 'sanitized_generation', 'fail_base', 'fail_plus']]\n",
    "\n",
    "        with open(f'crux-eval.github.io/eval-arena/evalplus/{id}.html', 'w') as f: \n",
    "            f.write(j2_template.render({'id': id, 'problem': first['problem'],\n",
    "                                        'canonical_solution': first['canonical_solution'],\n",
    "                                        'base_input': first['base_input'],\n",
    "                                        'plus_input': first['plus_input'],\n",
    "                                        'outputs': generation}))\n",
    "\n",
    "write_benchmark('humaneval+')\n",
    "write_benchmark('mbpp+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(benchmark_id, example_id):\n",
    "    # has these fields dict_keys(['task_id', 'prompt', 'entry_point', 'canonical_solution', 'test', 'contract', 'base_input', 'atol', 'plus_input'])\n",
    "    if benchmark_id == 'humaneval+':\n",
    "        get_bench = get_human_eval_plus\n",
    "    else:\n",
    "        get_bench = get_mbpp_plus\n",
    "\n",
    "    samples = [\n",
    "        dict(example_id=task_id, problem=problem['prompt'], solution=problem['canonical_solution'], test=problem['test'], plus_input=problem['plus_input'])\n",
    "        for task_id, problem in get_bench().items()\n",
    "    ]\n",
    "    df_prob = pd.DataFrame(samples)\n",
    "    for r in df_prob[df_prob['example_id'] == example_id].to_numpy():\n",
    "        for v in r:\n",
    "            print(v)\n",
    "\n",
    "inspect('humaneval+', 'HumanEval/122')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = pd.read_csv('data/lcb_arena.csv')\n",
    "display(dfi)\n",
    "with open('data/lcb_arena.jsonl', 'w') as f:\n",
    "    dfi = dfi[['benchmark_id', 'example_id', 'model', 'pass1']] \n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = pd.read_json('raw-data/ds1000-full.jsonl', lines=True)\n",
    "with open('data/ds1000.jsonl', 'w') as f:\n",
    "    dfi = dfi[['benchmark_id', 'example_id', 'model', 'pass1']] \n",
    "    f.write(dfi.to_json(orient='records', lines=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process evalplus ones\n",
    "def evalplus(name, isplus):\n",
    "    records = []\n",
    "    for fname in glob.glob(f\"data/noise_analysis/{name}/*.jsonl\"):\n",
    "        with open(fname, 'rt') as f:\n",
    "            records.extend([json.loads(l) for l in f.readlines()])\n",
    "    df = pd.DataFrame(records)\n",
    "    # display(df.describe())\n",
    "    if isplus:\n",
    "        df['pass1'] = np.where(df['fail_plus'], 0, 1) \n",
    "        df['benchmark_id'] = f'{name}+' \n",
    "    else:\n",
    "        df['pass1'] = np.where(df['fail_base'], 0, 1) \n",
    "        df['benchmark_id'] = f'{name}'\n",
    "    df = df[[\n",
    "        'benchmark_id',\n",
    "        'model',\n",
    "        'example_id',\n",
    "        'pass1',\n",
    "    ]]\n",
    "    return df\n",
    "\n",
    "with open('data/humaneval.jsonl', 'w') as f:\n",
    "    dfi = evalplus('humaneval', False)\n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))\n",
    "\n",
    "with open('data/humaneval+.jsonl', 'w') as f:\n",
    "    dfi = evalplus('humaneval', True)\n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))\n",
    "\n",
    "with open('data/mbpp.jsonl', 'w') as f:\n",
    "    dfi = evalplus('mbpp', False)\n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))\n",
    "\n",
    "with open('data/mbpp+.jsonl', 'w') as f:\n",
    "    dfi = evalplus('mbpp', True)\n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_llamma(bid, path, model_suf=''):\n",
    "    with open(path) as f:\n",
    "        lines = json.load(f)\n",
    "    records = []\n",
    "    for l in lines:\n",
    "        if 'metadata' in l and bid == 'DS1000':\n",
    "            eid = f\"DS/{l['metadata']['problem_id']}\"\n",
    "        elif 'id' in l and bid == 'CRUXEval-input':\n",
    "            eid = \"CRUXEval-input/\" + l['id'].split('_')[1]\n",
    "        elif 'id' in l and bid == 'CRUXEval-output':\n",
    "            eid = \"CRUXEval-output/\" + l['id'].split('_')[1]\n",
    "        else:\n",
    "            raise Exception('not supported') \n",
    "\n",
    "        records.append({\n",
    "            'benchmark_id': bid,\n",
    "            'example_id': eid,\n",
    "            'pass1': l['metrics']['pass@1'],\n",
    "            'model': f'llama3-X{model_suf}',\n",
    "        })\n",
    "    return records\n",
    "\n",
    "# cruxeval_input_chat.json  cruxeval_input_cot_chat.json  cruxeval_output_chat.json  cruxeval_output_cot_chat.json  ds1000_chat.json\n",
    "records = []\n",
    "records += process_llamma('DS1000', 'data/llama_result/ds1000_chat.json')\n",
    "records += process_llamma('CRUXEval-input', 'data/llama_result/cruxeval_input_chat.json')\n",
    "records += process_llamma('CRUXEval-input', 'data/llama_result/cruxeval_input_cot_chat.json', '-cot')\n",
    "records += process_llamma('CRUXEval-output', 'data/llama_result/cruxeval_output_chat.json')\n",
    "records += process_llamma('CRUXEval-output', 'data/llama_result/cruxeval_output_cot_chat.json', '-cot')\n",
    "dfi = pd.DataFrame(records)\n",
    "with open('data/llama3.jsonl', 'wt') as f:\n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = pd.read_csv('raw-data/hf_models/human_eval.csv')\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "# print(dfi[['model_family', 'model_name']].drop_duplicates())\n",
    "# print(set(dfi['model_family']))\n",
    "# filter not allowed and very bad models\n",
    "dfi = dfi[~dfi['model_family'].isin(['Llama 2', 'Llama 1', 'tiiuae', 'EleutherAI'])]\n",
    "maps = {'model_name': 'model', 'benchmark': 'benchmark_id', 'pass@1': 'pass1', 'example_idx': 'example_id'}\n",
    "dfi = dfi[maps.keys()].rename(columns=maps)\n",
    "dfi['example_id'] = dfi['example_id'].apply(lambda x: f'HumanEval/{x}')\n",
    "with open('raw-data/human_eval_hf.jsonl', 'w') as f:\n",
    "    dfi = dfi[['benchmark_id', 'example_id', 'model', 'pass1']] \n",
    "    f.write(dfi.to_json(orient='records', lines=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for path in glob.glob('raw-data/hf_models/*.csv'):\n",
    "    print(f'processing {path}')\n",
    "    dfi = pd.read_csv(path)\n",
    "    display(dfi.columns)\n",
    "    bid = bprefix = set(dfi['benchmark']).pop()\n",
    "    if bid == 'human_eval': # consistency of the overlap\n",
    "        dfi = dfi[~dfi['model_family'].isin(['Llama 2', 'Llama 1', 'tiiuae', 'EleutherAI'])]\n",
    "        bid = 'humaneval'\n",
    "        dfi['benchmark'] = 'humaneval'\n",
    "        bprefix = 'HumanEval'\n",
    "        display(dfi)\n",
    "    else:\n",
    "        dfi = dfi[~dfi['model_family'].isin(['Llama 2', 'Llama 1'])]\n",
    "\n",
    "    maps = {'model_name': 'model', 'benchmark': 'benchmark_id', 'example_idx': 'example_id'}\n",
    "    if 'pass@1' in dfi.columns:\n",
    "        maps['pass@1'] = 'pass1'\n",
    "    elif 'acc_raw' in dfi.columns:\n",
    "        maps['acc_raw'] = 'pass1'\n",
    "        display(dfi[['acc_raw', 'acc_char', 'acc_token']])\n",
    "        if not np.allclose(dfi['acc_raw'], dfi['acc_char'], rtol=1e-2):\n",
    "            print('inconsistent')\n",
    "    elif 'em' in dfi.columns:\n",
    "        maps['em'] = 'pass1'\n",
    "\n",
    "    dfi = dfi[maps.keys()].rename(columns=maps)\n",
    "    display(dfi.describe())\n",
    "    \n",
    "    dfi['example_id'] = dfi['example_id'].apply(lambda x: f'{bprefix}/{x}')\n",
    "    display(dfi)\n",
    "    with open(f'raw-data/{bid}_hf.jsonl', 'w') as f:\n",
    "        dfi = dfi[['benchmark_id', 'example_id', 'model', 'pass1']] \n",
    "        f.write(dfi.to_json(orient='records', lines=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in glob.glob('raw-data/hf_models/human_eval.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    display(df.groupby('model_name').agg('mean', numeric_only=True).reset_index()[['model_name', 'pass@1']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codegen_240116_sida",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
