{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data formatting scripts\n",
    "\n",
    "from collections import defaultdict\n",
    "import json, math, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_swe(swetype, id_path):\n",
    "    df = pd.read_json(id_path, lines=True, orient='records')\n",
    "    ids = set(df['instance_id'])\n",
    "    print(f'len of {swetype}', len(ids))\n",
    "    print('model', 'total', 'deduped')\n",
    "    for fname in glob.glob(f\"raw-data/swebench-experiments/evaluation/{swetype}/*/all_preds.jsonl\"):\n",
    "        mname = fname.split('/')[-2]\n",
    "        try:\n",
    "            df = pd.read_json(fname, lines=True, orient='records')\n",
    "        except:\n",
    "            print('not jsonl', fname)\n",
    "\n",
    "        print(mname, len(df), len(set(df['instance_id'])))\n",
    "\n",
    "    records = []\n",
    "    for fname in glob.glob(f\"raw-data/swebench-experiments/evaluation/{swetype}/*/results/results.json\"):\n",
    "        # print(fname)\n",
    "        mname = fname.split('/')[-3]\n",
    "        print(mname)\n",
    "        with open(fname, 'r') as f:\n",
    "            result = json.load(f)\n",
    "        if 'resolved' in result:\n",
    "            resolved = set(result['resolved'])\n",
    "        else: resolved = set()\n",
    "        print(resolved)\n",
    "        \n",
    "        for id in ids:\n",
    "            records.append({\n",
    "                'benchmark_id': f'swebench-{swetype}',\n",
    "                'model': mname,\n",
    "                'example_id': id,\n",
    "                'pass1': 1 if id in resolved else 0\n",
    "            })\n",
    "    dfo = pd.DataFrame(records)\n",
    "    display(dfo)\n",
    "    dfo.to_json(f'data/swebench-{swetype}.jsonl', orient='records', lines=True)\n",
    "\n",
    "swetype = 'lite'\n",
    "id_path = 'raw-data/swebench-experiments/evaluation/lite/20231010_rag_claude2/all_preds.jsonl'\n",
    "process_swe(swetype, id_path)\n",
    "\n",
    "swetype = 'verified'\n",
    "id_path = 'raw-data/swebench-experiments/evaluation/verified/20231010_rag_claude2/all_preds.jsonl'\n",
    "process_swe(swetype, id_path)\n",
    "\n",
    "swetype = 'test'\n",
    "id_path = 'raw-data/swebench-experiments/evaluation/test/20231010_rag_claude2/all_preds.jsonl'\n",
    "process_swe(swetype, id_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cruxeval(type):\n",
    "    records = []\n",
    "    for fname in glob.glob(f\"crux-eval.github.io/evaluation_results/*_temp0.2_{type}.json\"):\n",
    "        name = fname.split('/')[-1]\n",
    "        model, temp, typejsonl = name.split('_')\n",
    "        print(model, temp, type)\n",
    "\n",
    "        with open(fname) as f:\n",
    "            res = json.load(f)['raw_scored_generations']\n",
    "            for exid in res:\n",
    "                gotid = np.mean(res[exid])\n",
    "                actualid = exid.replace('sample_', '')\n",
    "                records.append({\n",
    "                    'benchmark_id': f'CRUXEval-{type}',\n",
    "                    'model': model,\n",
    "                    'example_id': f\"CRUXEval-{type}/{actualid}\",\n",
    "                    'pass1': gotid,\n",
    "                    'hyperparams': 'temp0.2'\n",
    "                })\n",
    "    df = pd.DataFrame(records)\n",
    "    display(df.describe())\n",
    "    return df\n",
    "        \n",
    "with open('data/cruxeval_input.jsonl', 'w') as f:\n",
    "    dfi = get_cruxeval('input')\n",
    "    f.write(dfi.to_json(orient='records', lines=True))\n",
    "\n",
    "with open('data/cruxeval_output.jsonl', 'w') as f:\n",
    "    dfo = get_cruxeval('output')\n",
    "    f.write(dfo.to_json(orient='records', lines=True))\n",
    "\n",
    "pass1 = pd.concat([dfi, dfo])\n",
    "display(pass1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate evalplus links\n",
    "\n",
    "import sys\n",
    "from jinja2 import Template\n",
    "\n",
    "sys.path.append('/private/home/sida/git/CodeGen/scripts/sida/arena/data/evalplus')\n",
    "\n",
    "from evalplus.data import get_mbpp_plus, get_human_eval_plus, write_jsonl\n",
    "# has these fields dict_keys(['task_id', 'prompt', 'entry_point', 'canonical_solution', 'test', 'contract', 'base_input', 'atol', 'plus_input'])\n",
    "\n",
    "j2_template = Template(\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\" />   <!--It is necessary to use the UTF-8 encoding with plotly graphics to get e.g. negative signs to render correctly -->\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "<link\n",
    "  rel=\"stylesheet\"\n",
    "  href=\"https://crux-eval.github.io/static/css/bulma.min.css\"\n",
    ">\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "<section class=\"section\">                       \n",
    "<h1> {{ id }} </h1>\n",
    "<pre><code> {{ problem }} </code></pre>\n",
    "<h2>solution</h2>\n",
    "<pre><code> {{ canonical_solution }} </code></pre>\n",
    "<h2>base input</h2>\n",
    "<pre><code> {{ base_input }} </code></pre>\n",
    "<h2>plus input</h2>\n",
    "<pre><code> {{ plus_input }} </code></pre>\n",
    "<br>\n",
    "<h1>Generations</h1>\n",
    "{% for key,value in outputs.iterrows() %}\n",
    "      <p> {{ value['model'] }}. fail_base: {{ value['fail_base'] }}, fail_plus: {{ value['fail_plus'] }} </p>\n",
    "      <pre><code>  {{ value['raw_genereation'] }}</code></pre>\n",
    "{% endfor %}\n",
    "                       \n",
    "</section>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\")\n",
    "\n",
    "def write_benchmark(benchmark_id):\n",
    "    if benchmark_id == 'humaneval+':\n",
    "        get_bench = get_human_eval_plus\n",
    "    else:\n",
    "        get_bench = get_mbpp_plus\n",
    "\n",
    "    samples = [\n",
    "        dict(example_id=task_id,\n",
    "            problem=problem['prompt'],\n",
    "            canonical_solution=problem['canonical_solution'],\n",
    "            base_input=problem['base_input'],\n",
    "            plus_input=problem['plus_input'],\n",
    "        )\n",
    "        for task_id, problem in get_bench().items()\n",
    "    ]\n",
    "    df_prob = pd.DataFrame(samples)\n",
    "\n",
    "    records = []\n",
    "    for fname in glob.glob(f\"data/noise_analysis/*/*.jsonl\"):\n",
    "        with open(fname, 'rt') as f:\n",
    "            records.extend([json.loads(l) for l in f.readlines()])\n",
    "\n",
    "    eval_results = pd.DataFrame(records)\n",
    "    display(eval_results.describe())\n",
    "\n",
    "    benchmark_results = eval_results[eval_results['benchmark_id'] == benchmark_id]\n",
    "\n",
    "    df = df_prob.merge(benchmark_results, on='example_id')\n",
    "\n",
    "    ids = list(set(df['example_id']))\n",
    "    for id in ids:\n",
    "        current_prob = df[df['example_id'] == id]\n",
    "        first = current_prob.iloc[0]\n",
    "        # display(current_prob)\n",
    "        generation = current_prob[['model', 'raw_genereation', 'sanitized_generation', 'fail_base', 'fail_plus']]\n",
    "\n",
    "        with open(f'crux-eval.github.io/eval-arena/evalplus/{id}.html', 'w') as f: \n",
    "            f.write(j2_template.render({'id': id, 'problem': first['problem'],\n",
    "                                        'canonical_solution': first['canonical_solution'],\n",
    "                                        'base_input': first['base_input'],\n",
    "                                        'plus_input': first['plus_input'],\n",
    "                                        'outputs': generation}))\n",
    "\n",
    "write_benchmark('humaneval+')\n",
    "write_benchmark('mbpp+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(benchmark_id, example_id):\n",
    "    # has these fields dict_keys(['task_id', 'prompt', 'entry_point', 'canonical_solution', 'test', 'contract', 'base_input', 'atol', 'plus_input'])\n",
    "    if benchmark_id == 'humaneval+':\n",
    "        get_bench = get_human_eval_plus\n",
    "    else:\n",
    "        get_bench = get_mbpp_plus\n",
    "\n",
    "    samples = [\n",
    "        dict(example_id=task_id, problem=problem['prompt'], solution=problem['canonical_solution'], test=problem['test'], plus_input=problem['plus_input'])\n",
    "        for task_id, problem in get_bench().items()\n",
    "    ]\n",
    "    df_prob = pd.DataFrame(samples)\n",
    "    for r in df_prob[df_prob['example_id'] == example_id].to_numpy():\n",
    "        for v in r:\n",
    "            print(v)\n",
    "\n",
    "inspect('humaneval+', 'HumanEval/122')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = pd.read_csv('data/lcb_arena.csv')\n",
    "display(dfi)\n",
    "with open('data/lcb_arena.jsonl', 'w') as f:\n",
    "    dfi = dfi[['benchmark_id', 'example_id', 'model', 'pass1']] \n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = pd.read_json('raw-data/ds1000-full.jsonl', lines=True)\n",
    "with open('data/ds1000.jsonl', 'w') as f:\n",
    "    dfi = dfi[['benchmark_id', 'example_id', 'model', 'pass1']] \n",
    "    f.write(dfi.to_json(orient='records', lines=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process evalplus ones\n",
    "def evalplus(name, isplus):\n",
    "    records = []\n",
    "    for fname in glob.glob(f\"data/noise_analysis/{name}/*.jsonl\"):\n",
    "        with open(fname, 'rt') as f:\n",
    "            records.extend([json.loads(l) for l in f.readlines()])\n",
    "    df = pd.DataFrame(records)\n",
    "    # display(df.describe())\n",
    "    if isplus:\n",
    "        df['pass1'] = np.where(df['fail_plus'], 0, 1) \n",
    "        df['benchmark_id'] = f'{name}+' \n",
    "    else:\n",
    "        df['pass1'] = np.where(df['fail_base'], 0, 1) \n",
    "        df['benchmark_id'] = f'{name}'\n",
    "    df = df[[\n",
    "        'benchmark_id',\n",
    "        'model',\n",
    "        'example_id',\n",
    "        'pass1',\n",
    "    ]]\n",
    "    return df\n",
    "\n",
    "with open('data/humaneval.jsonl', 'w') as f:\n",
    "    dfi = evalplus('humaneval', False)\n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))\n",
    "\n",
    "with open('data/humaneval+.jsonl', 'w') as f:\n",
    "    dfi = evalplus('humaneval', True)\n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))\n",
    "\n",
    "with open('data/mbpp.jsonl', 'w') as f:\n",
    "    dfi = evalplus('mbpp', False)\n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))\n",
    "\n",
    "with open('data/mbpp+.jsonl', 'w') as f:\n",
    "    dfi = evalplus('mbpp', True)\n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codegen_240613_loc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
