{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data formatting scripts\n",
    "from collections import defaultdict\n",
    "import json, math, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# sys.path.append('raw-data/lcb_submissions')\n",
    "# sys.path.append('raw-data/LiveCodeBench')\n",
    "\n",
    "# # %cd raw-data/lcb_submissions/\n",
    "\n",
    "def load_performances_generation(version=6):\n",
    "    results = []\n",
    "    # Get current directory to restore later\n",
    "    original_dir = os.getcwd()\n",
    "    \n",
    "    os.chdir('raw-data/lcb_submissions')\n",
    "\n",
    "    for model in glob.glob(\"*/Scenario.codegeneration_*_eval_all.json\"):\n",
    "        mname = model.split(\"/\")[0]\n",
    "        print(mname)\n",
    "        fnames = [\n",
    "            f\"{mname}/Scenario.codegeneration_*_eval_all.json\"\n",
    "            # for i in range(9)\n",
    "        ]\n",
    "        fname = sum([glob.glob(fname) for fname in fnames], [])\n",
    "\n",
    "        if not fname:\n",
    "            print(fnames, \"not found\")\n",
    "            # print(f\"{fname} does not exist\")\n",
    "            fname = f\".json\"\n",
    "            fname = glob.glob(fname)\n",
    "            if not fname:\n",
    "                # print(f\"{fname} does not exist\")\n",
    "                continue\n",
    "            else:\n",
    "                assert len(fname) == 1\n",
    "            fname = fname[0]\n",
    "        else:\n",
    "            if len(fname) != 1:\n",
    "                continue\n",
    "            fname = fname[0]\n",
    "\n",
    "        with open(fname) as fp:\n",
    "            model_outputs = json.load(fp)\n",
    "        \n",
    "        lengths = {5: 880, 6:1055}\n",
    "        if len(model_outputs) != lengths[v]:\n",
    "            continue\n",
    "        # assert (\n",
    "        #     model.release_date is not None\n",
    "        # ), f\"Model {model.model_repr} has no release date\"\n",
    "\n",
    "        results.extend(\n",
    "            [\n",
    "                {\n",
    "                    # \"model_class\": model,\n",
    "                    \"question_id\": model_output[\"question_id\"],\n",
    "                    \"model\": mname,\n",
    "                    \"date\": datetime.fromisoformat(model_output[\"contest_date\"]),\n",
    "                    \"difficulty\": model_output[\"difficulty\"],\n",
    "                    \"N\": len(model_output[\"graded_list\"]),\n",
    "                    \"pass@1\": (\n",
    "                        model_output[\"pass1\"] * 100\n",
    "                        if \"pass1\" in model_output\n",
    "                        else model_output[\"pass@1\"] * 100\n",
    "                    ),\n",
    "                    \"platform\": (\n",
    "                        \"leetcode\"\n",
    "                        if isinstance(model_output[\"question_id\"], int)\n",
    "                        else (\n",
    "                            \"codeforces\"\n",
    "                            if model_output[\"question_id\"][0] == \"1\"\n",
    "                            else model_output[\"platform\"]\n",
    "                        )\n",
    "                    ),\n",
    "                }\n",
    "                for model_output in model_outputs\n",
    "            ]\n",
    "        )\n",
    "    df = pd.DataFrame(results)\n",
    "    # print(df.head())\n",
    "    os.chdir(original_dir)\n",
    "    return df\n",
    "\n",
    "\n",
    "for v in [5, 6]:\n",
    "    df = load_performances_generation(version=v)\n",
    "    display(df)\n",
    "    dfo = df.copy()\n",
    "    dfo['benchmark_id'] = f'lcb_codegen_v{v}'\n",
    "    dfo['example_id'] = dfo['platform'] + '.' + dfo['question_id']\n",
    "    dfo['pass1'] = dfo['pass@1'] / 100.0\n",
    "    dfo = dfo[['benchmark_id', 'model', 'example_id', 'pass1', \"N\", \"date\"]]\n",
    "    dfo.to_json(f'data/lcb_codegen_v{v}.jsonl', orient='records', lines=True)\n",
    "    display(dfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_performances_generation(version=6)\n",
    "display(df)\n",
    "df = df[df[\"date\"] > datetime.fromisoformat(\"2024-08-01\")]\n",
    "display(df.groupby(\"model\").agg(\"count\"))\n",
    "dfo = df.copy()\n",
    "dfo['benchmark_id'] = f'lcb_codegen_v{v}_080124'\n",
    "dfo['example_id'] = dfo['platform'] + '.' + dfo['question_id']\n",
    "dfo['pass1'] = dfo['pass@1'] / 100.0\n",
    "dfo = dfo[['benchmark_id', 'model', 'example_id', 'pass1', \"N\", \"date\"]]\n",
    "dfo.to_json(f'data/lcb_codegen_v{v}_080124.jsonl', orient='records', lines=True)\n",
    "display(dfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "# Get current directory to restore later\n",
    "original_dir = os.getcwd()\n",
    "\n",
    "try:\n",
    "    # Change to the lcb_submissions directory\n",
    "    os.chdir('raw-data/lcb_submissions')\n",
    "    # Find all JSON files and count entries\n",
    "    for json_file in glob.glob('*/*.json'):\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    count = len(data)\n",
    "                elif isinstance(data, dict):\n",
    "                    count = len(data)\n",
    "                else:\n",
    "                    count = 1\n",
    "                print(f\"{json_file}: {count} entries\")\n",
    "        except Exception as e:\n",
    "            print(f\"{json_file}: Error reading file - {e}\")\n",
    "            \n",
    "finally:\n",
    "    # Restore original directory\n",
    "    os.chdir(original_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_swe(swetype, id_path):\n",
    "    with open(id_path) as id_file:\n",
    "        res = json.load(id_file)\n",
    "    print(res)\n",
    "    ids = set(res['generated']) | set(res['no_generation'])\n",
    "    print(f'len of {swetype}', len(ids))\n",
    "    print('model', 'total', 'deduped')\n",
    "    for fname in glob.glob(f\"raw-data/swebench-experiments/evaluation/{swetype}/*/results/results.json\"):\n",
    "        mname = fname.split('/')[-3]\n",
    "        try:\n",
    "            with open(id_path) as id_file:\n",
    "                res = json.load(id_file)\n",
    "        except:\n",
    "            print('not jsonl', fname)\n",
    "        total_list = res[\"generated\"] + res[\"no_generation\"]\n",
    "        print(mname, len(total_list), len(set(total_list)))\n",
    "        assert len(total_list) == len(ids), \"results for {fname} is incomplete\"\n",
    "\n",
    "    records = []\n",
    "    for fname in glob.glob(f\"raw-data/swebench-experiments/evaluation/{swetype}/*/results/results.json\"):\n",
    "        # print(fname)\n",
    "        mname = fname.split('/')[-3]\n",
    "        print(mname)\n",
    "        with open(fname, 'r') as f:\n",
    "            result = json.load(f)\n",
    "        if 'resolved' in result:\n",
    "            resolved = set(result['resolved'])\n",
    "        else: resolved = set()\n",
    "        \n",
    "        for id in ids:\n",
    "            records.append({\n",
    "                'benchmark_id': f'swebench-{swetype}',\n",
    "                'model': mname,\n",
    "                'example_id': id,\n",
    "                'pass1': 1 if id in resolved else 0\n",
    "            })\n",
    "    dfo = pd.DataFrame(records)\n",
    "    display(dfo)\n",
    "    dfo.to_json(f'data/swebench-{swetype}.jsonl', orient='records', lines=True)\n",
    "\n",
    "swetype = 'lite'\n",
    "id_path = 'raw-data/swebench-experiments/evaluation/lite/20231010_rag_claude2/results/results.json'\n",
    "process_swe(swetype, id_path)\n",
    "\n",
    "swetype = 'verified'\n",
    "id_path = 'raw-data/swebench-experiments/evaluation/verified/20231010_rag_claude2/results/results.json'\n",
    "process_swe(swetype, id_path)\n",
    "\n",
    "swetype = 'test'\n",
    "id_path = 'raw-data/swebench-experiments/evaluation/test/20231010_rag_claude2/results/results.json'\n",
    "process_swe(swetype, id_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cruxeval(type, temp=\"0.2\"):\n",
    "    records = []\n",
    "    for fname in glob.glob(f\"crux-eval.github.io/evaluation_results/*_temp{temp}_{type}.json\"):\n",
    "        name = fname.split('/')[-1]\n",
    "        model, temps, typejsonl = name.split('_')\n",
    "        print(model, temps, type)\n",
    "\n",
    "        with open(fname) as f:\n",
    "            res = json.load(f)['raw_scored_generations']\n",
    "            for exid in res:\n",
    "                gotid = np.mean(res[exid])\n",
    "                N = len(res[exid])\n",
    "                actualid = exid.replace('sample_', '')\n",
    "                records.append({\n",
    "                    'benchmark_id': f'CRUXEval-{type}-T{temp}',\n",
    "                    'model': model,\n",
    "                    'example_id': f\"CRUXEval-{type}/{actualid}\",\n",
    "                    'pass1': gotid,\n",
    "                    'N': N,\n",
    "                    'hyperparams': f'T{temp}'\n",
    "                })\n",
    "    df = pd.DataFrame(records)\n",
    "    display(df.describe())\n",
    "    return df\n",
    "\n",
    "def input_output(temp=\"0.2\"):\n",
    "    with open(f'data/cruxeval_input_T{temp}.jsonl', 'w') as f:\n",
    "        dfi = get_cruxeval('input', temp=temp)\n",
    "        f.write(dfi.to_json(orient='records', lines=True))\n",
    "\n",
    "    with open(f'data/cruxeval_output_T{temp}.jsonl', 'w') as f:\n",
    "        dfo = get_cruxeval('output', temp=temp)\n",
    "        f.write(dfo.to_json(orient='records', lines=True))\n",
    "\n",
    "    pass1 = pd.concat([dfi, dfo])\n",
    "    display(pass1)\n",
    "\n",
    "input_output(\"0.2\")\n",
    "input_output(\"0.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate evalplus links\n",
    "\n",
    "import sys\n",
    "from jinja2 import Template\n",
    "\n",
    "sys.path.append('/private/home/sida/git/CodeGen/scripts/sida/arena/data/evalplus')\n",
    "\n",
    "from evalplus.data import get_mbpp_plus, get_human_eval_plus, write_jsonl\n",
    "# has these fields dict_keys(['task_id', 'prompt', 'entry_point', 'canonical_solution', 'test', 'contract', 'base_input', 'atol', 'plus_input'])\n",
    "\n",
    "j2_template = Template(\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\" />   <!--It is necessary to use the UTF-8 encoding with plotly graphics to get e.g. negative signs to render correctly -->\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "<link\n",
    "  rel=\"stylesheet\"\n",
    "  href=\"https://crux-eval.github.io/static/css/bulma.min.css\"\n",
    ">\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "<section class=\"section\">                       \n",
    "<h1> {{ id }} </h1>\n",
    "<pre><code> {{ problem }} </code></pre>\n",
    "<h2>solution</h2>\n",
    "<pre><code> {{ canonical_solution }} </code></pre>\n",
    "<h2>base input</h2>\n",
    "<pre><code> {{ base_input }} </code></pre>\n",
    "<h2>plus input</h2>\n",
    "<pre><code> {{ plus_input }} </code></pre>\n",
    "<br>\n",
    "<h1>Generations</h1>\n",
    "{% for key,value in outputs.iterrows() %}\n",
    "      <p> {{ value['model'] }}. fail_base: {{ value['fail_base'] }}, fail_plus: {{ value['fail_plus'] }} </p>\n",
    "      <pre><code>  {{ value['raw_genereation'] }}</code></pre>\n",
    "{% endfor %}\n",
    "                       \n",
    "</section>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\")\n",
    "\n",
    "def write_benchmark(benchmark_id):\n",
    "    if benchmark_id == 'humaneval+':\n",
    "        get_bench = get_human_eval_plus\n",
    "    else:\n",
    "        get_bench = get_mbpp_plus\n",
    "\n",
    "    samples = [\n",
    "        dict(example_id=task_id,\n",
    "            problem=problem['prompt'],\n",
    "            canonical_solution=problem['canonical_solution'],\n",
    "            base_input=problem['base_input'],\n",
    "            plus_input=problem['plus_input'],\n",
    "        )\n",
    "        for task_id, problem in get_bench().items()\n",
    "    ]\n",
    "    df_prob = pd.DataFrame(samples)\n",
    "\n",
    "    records = []\n",
    "    for fname in glob.glob(f\"data/noise_analysis/*/*.jsonl\"):\n",
    "        with open(fname, 'rt') as f:\n",
    "            records.extend([json.loads(l) for l in f.readlines()])\n",
    "\n",
    "    eval_results = pd.DataFrame(records)\n",
    "    display(eval_results.describe())\n",
    "\n",
    "    benchmark_results = eval_results[eval_results['benchmark_id'] == benchmark_id]\n",
    "\n",
    "    df = df_prob.merge(benchmark_results, on='example_id')\n",
    "\n",
    "    ids = list(set(df['example_id']))\n",
    "    for id in ids:\n",
    "        current_prob = df[df['example_id'] == id]\n",
    "        first = current_prob.iloc[0]\n",
    "        # display(current_prob)\n",
    "        generation = current_prob[['model', 'raw_genereation', 'sanitized_generation', 'fail_base', 'fail_plus']]\n",
    "\n",
    "        with open(f'crux-eval.github.io/eval-arena/evalplus/{id}.html', 'w') as f: \n",
    "            f.write(j2_template.render({'id': id, 'problem': first['problem'],\n",
    "                                        'canonical_solution': first['canonical_solution'],\n",
    "                                        'base_input': first['base_input'],\n",
    "                                        'plus_input': first['plus_input'],\n",
    "                                        'outputs': generation}))\n",
    "\n",
    "write_benchmark('humaneval+')\n",
    "write_benchmark('mbpp+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(benchmark_id, example_id):\n",
    "    # has these fields dict_keys(['task_id', 'prompt', 'entry_point', 'canonical_solution', 'test', 'contract', 'base_input', 'atol', 'plus_input'])\n",
    "    if benchmark_id == 'humaneval+':\n",
    "        get_bench = get_human_eval_plus\n",
    "    else:\n",
    "        get_bench = get_mbpp_plus\n",
    "\n",
    "    samples = [\n",
    "        dict(example_id=task_id, problem=problem['prompt'], solution=problem['canonical_solution'], test=problem['test'], plus_input=problem['plus_input'])\n",
    "        for task_id, problem in get_bench().items()\n",
    "    ]\n",
    "    df_prob = pd.DataFrame(samples)\n",
    "    for r in df_prob[df_prob['example_id'] == example_id].to_numpy():\n",
    "        for v in r:\n",
    "            print(v)\n",
    "\n",
    "inspect('humaneval+', 'HumanEval/122')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = pd.read_csv('data/lcb_arena.csv')\n",
    "display(dfi)\n",
    "with open('data/lcb_arena.jsonl', 'w') as f:\n",
    "    dfi = dfi[['benchmark_id', 'example_id', 'model', 'pass1']] \n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = pd.read_json('raw-data/ds1000-full.jsonl', lines=True)\n",
    "with open('data/ds1000.jsonl', 'w') as f:\n",
    "    dfi = dfi[['benchmark_id', 'example_id', 'model', 'pass1']] \n",
    "    f.write(dfi.to_json(orient='records', lines=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process evalplus ones\n",
    "def evalplus(name, isplus):\n",
    "    records = []\n",
    "    for fname in glob.glob(f\"data/noise_analysis/{name}/*.jsonl\"):\n",
    "        with open(fname, 'rt') as f:\n",
    "            records.extend([json.loads(l) for l in f.readlines()])\n",
    "    df = pd.DataFrame(records)\n",
    "    # display(df.describe())\n",
    "    if isplus:\n",
    "        df['pass1'] = np.where(df['fail_plus'], 0, 1) \n",
    "        df['benchmark_id'] = f'{name}+' \n",
    "    else:\n",
    "        df['pass1'] = np.where(df['fail_base'], 0, 1) \n",
    "        df['benchmark_id'] = f'{name}'\n",
    "    df = df[[\n",
    "        'benchmark_id',\n",
    "        'model',\n",
    "        'example_id',\n",
    "        'pass1',\n",
    "    ]]\n",
    "    return df\n",
    "\n",
    "with open('data/humaneval.jsonl', 'w') as f:\n",
    "    dfi = evalplus('humaneval', False)\n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))\n",
    "\n",
    "with open('data/humaneval+.jsonl', 'w') as f:\n",
    "    dfi = evalplus('humaneval', True)\n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))\n",
    "\n",
    "with open('data/mbpp.jsonl', 'w') as f:\n",
    "    dfi = evalplus('mbpp', False)\n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))\n",
    "\n",
    "with open('data/mbpp+.jsonl', 'w') as f:\n",
    "    dfi = evalplus('mbpp', True)\n",
    "    f.write(dfi.to_json(orient='records', lines=True, index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfi = pd.read_csv(\"/checkpoint/amaia/explore/obriensean/share/0813_cot_more/results.csv\")\n",
    "dfi = pd.read_csv(\"/checkpoint/amaia/explore/obriensean/amaia_dumps/0926_evals_share/temp1/results.csv\")\n",
    "display(dfi.describe())\n",
    "display(dfi[[\"model\", \"task_name\"]].describe())\n",
    "df = dfi.rename(columns={\"task_name\": \"benchmark_id\", \"question_idx\": \"example_id\", 'model': 'model', \"count\": \"N\", \"pass\": \"pass1\"})\n",
    "display(set(df[\"benchmark_id\"]))\n",
    "df[\"example_id\"] = df[\"example_id\"].apply(lambda x: str(x))\n",
    "df[\"benchmark_id\"] = df[\"benchmark_id\"].apply(lambda x: x if x != \"mbpp\" else \"mbpp_vllm\")\n",
    "# with open('data/vLLM_results_T1.jsonl', 'w') as f:\n",
    "#     f.write(df.to_json(orient='records', lines=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df)\n",
    "sel = df[(df[\"model\"] == \"llama-3.1-8B-instruct\") & (df[\"benchmark_id\"] == \"human_eval_plus\")]\n",
    "display(sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(set(df[\"model\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codegen_240613_loc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
