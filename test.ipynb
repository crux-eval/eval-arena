{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, math, glob\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from arena import result_table, pass1_to_battle, compute_pairwise_win_fraction, compute_pvalues \n",
    "\n",
    "records = []\n",
    "for fname in glob.glob(f\"data/*.jsonl\"):\n",
    "    with open(fname, 'rt') as f:\n",
    "        records.extend([json.loads(l) for l in f.readlines()])\n",
    "\n",
    "eval_results = pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pfunc(input: pd.Series):\n",
    "    sufs = Counter(input.values) # model_a, model_b, neither, both\n",
    "    res = {} \n",
    "    res['diff'] = sufs['model_a'] - sufs['model_b']\n",
    "    res['sum'] = sufs['model_a'] + sufs['model_b'] \n",
    "    # res['pvalue-chi2'] = 1 if res['diff'] == 0 else (1 - stats.chi2.cdf( (np.abs(res['diff']) - 1)**2 / res['sum'], 1))\n",
    "    res['pvalue-binom'] = stats.binomtest(sufs['model_a'], res['sum'], p=0.5).pvalue\n",
    "    p = sufs['model_a'] / res['sum']\n",
    "    p = p if p > 0.5 else 1-p\n",
    "    res['wrong_chance'] = stats.binomtest(res['sum']//2, res['sum'], p=p, alternative='less').pvalue\n",
    "    # res['pvalue-trinom'] = trinomial(sufs['model_a'], sufs['model_b'], sufs['both'] + sufs['neither'])\n",
    "    return res\n",
    "#suf_stats\n",
    "\n",
    "# for b in ['mbpp', 'humaneval', 'CRUXEval-input']:\n",
    "for b in ['humaneval', 'humaneval+']:\n",
    "    results = eval_results[eval_results['benchmark_id'] == b]\n",
    "    # models = list(set(results['model']))[:8]\n",
    "    # results = results[results['model'].isin(models)]\n",
    "    battles = pass1_to_battle(results)\n",
    "    # display(battles)\n",
    "    df = result_table(battles, results)\n",
    "    diffvsum = battles[['model_a', 'model_b', 'winner']].groupby(['model_a', 'model_b']).aggregate({'winner': pfunc})\n",
    "    diffvsum = diffvsum['winner'].apply(pd.Series)\n",
    "    diffvsum = diffvsum.reset_index(drop=False)\n",
    "    display(diffvsum)\n",
    "    figs = px.scatter(diffvsum, x=diffvsum['diff'].abs(), y='sum', custom_data=['model_a', 'model_b', 'sum', 'diff', 'pvalue-binom', 'wrong_chance'])\n",
    "    figs.update_traces(hovertemplate=\n",
    "        \"<br>\".join([\n",
    "        \"Model A: %{customdata[0]}\",\n",
    "        \"Model B: %{customdata[1]}\", \n",
    "        \"|A - B|: %{customdata[3]}\", \n",
    "        \"A + B: %{customdata[2]}\", \n",
    "        \"p-value: %{customdata[4]:.4f}\", \n",
    "        \"wrong prob.: %{customdata[5]:.4f}\", \n",
    "        ])  + '<extra></extra>')\n",
    "    \n",
    "    dsz = len(set(results['example_id']))\n",
    "    maxy = diffvsum['sum'].max()\n",
    "    print('maxy', maxy)\n",
    "\n",
    "    refs = []\n",
    "    for alpha in [0.05, 0.1]:\n",
    "        thres = stats.chi2.ppf(1-alpha, 1)\n",
    "        print('thres', thres)\n",
    "        y = np.linspace(1, maxy, 200)\n",
    "        refs.append(pd.DataFrame({'x': 1 + np.sqrt(y * thres), 'y': y, 'type': f'pvalue={alpha}'}))\n",
    "    x = np.linspace(0, dsz / 2, 100)\n",
    "    refs.append(pd.DataFrame({'x': x, 'y': x, 'type': 'x=y'}))\n",
    "    df_ref = pd.concat(refs, axis=0)\n",
    "    figl = px.line(df_ref, x='x', y='y', color='type', hover_data=[])\n",
    "    fig = go.Figure(data=figl.data + figs.data)\n",
    "    fig.update_layout(\n",
    "        width=600, height=600, title=b,\n",
    "        xaxis_title=\"|#A_win - #B_win|\",\n",
    "        yaxis_title=\"#A_win + #B_win\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pfunc(input: pd.Series):\n",
    "    sufs = Counter(input.values) # model_a, model_b, neither, both\n",
    "    res = {} \n",
    "    total = sufs.total()\n",
    "    res['diff'] = sufs['model_a'] - sufs['model_b']\n",
    "    res['sum'] = sufs['model_a'] + sufs['model_b'] \n",
    "    res['accA'] = (sufs['model_a'] + sufs['both']) / total\n",
    "    res['accB'] = (sufs['model_b'] + sufs['both']) / total\n",
    "    # res['pvalue-chi2'] = 1 if res['diff'] == 0 else (1 - stats.chi2.cdf( (np.abs(res['diff']) - 1)**2 / res['sum'], 1))\n",
    "    pv = stats.binomtest(sufs['model_a'], res['sum'], p=0.5).pvalue\n",
    "    res['p_value'] = pv if pv < 0.2 else 0.2\n",
    "    p = sufs['model_a'] / res['sum']\n",
    "    p = p if p > 0.5 else 1-p\n",
    "    res['wrong_chance'] = stats.binomtest(res['sum']//2, res['sum'], p=p, alternative='less').pvalue\n",
    "    # res['pvalue-trinom'] = trinomial(sufs['model_a'], sufs['model_b'], sufs['both'] + sufs['neither'])\n",
    "    return res\n",
    "#suf_stats\n",
    "\n",
    "# for b in ['mbpp', 'humaneval', 'CRUXEval-input']:\n",
    "for b in ['humaneval', 'humaneval+']:\n",
    "    results = eval_results[eval_results['benchmark_id'] == b]\n",
    "    # models = list(set(results['model']))[:8]\n",
    "    # results = results[results['model'].isin(models)]\n",
    "    battles = pass1_to_battle(results)\n",
    "    # display(battles)\n",
    "    df = result_table(battles, results)\n",
    "    diffvsum = battles[['model_a', 'model_b', 'winner']].groupby(['model_a', 'model_b']).aggregate({'winner': pfunc})\n",
    "    diffvsum = diffvsum['winner'].apply(pd.Series)\n",
    "    diffvsum = diffvsum.reset_index(drop=False)\n",
    "    display(diffvsum)\n",
    "    figs = px.scatter(diffvsum, x='accA', y='accB', color='p_value',\n",
    "        custom_data=['model_a', 'model_b', 'p_value', 'wrong_chance', 'accA', 'accB'])\n",
    "    figs.update_traces(hovertemplate=\n",
    "        \"<br>\".join([\n",
    "        \"Model A: %{customdata[0]}\",\n",
    "        \"Model B: %{customdata[1]}\", \n",
    "        \"acc(A): %{customdata[4]:.3f}\", \n",
    "        \"acc(B): %{customdata[5]:.3f}\", \n",
    "        \"p-value: %{customdata[2]:.4f}\", \n",
    "        \"wrong prob.: %{customdata[3]:.4f}\", \n",
    "        ])  + '<extra></extra>')\n",
    "    \n",
    "    dsz = len(set(results['example_id']))\n",
    "    maxy = diffvsum['sum'].max()\n",
    "    print('maxy', maxy)\n",
    "\n",
    "   \n",
    "    # fig = go.Figure(data=figs.data)\n",
    "    figs.update_layout(\n",
    "        width=600, height=600, title=b,\n",
    "        xaxis_title=\"acc(Model A)\",\n",
    "        yaxis_title=\"acc(Model B)\",\n",
    "        legend_title='p_value'\n",
    "    )\n",
    "    display(figs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import dblquad\n",
    "from scipy.special import gamma\n",
    "\n",
    "def beta_n(x, ax, bx):\n",
    "    return gamma(ax + bx) / gamma(ax) / gamma(bx) * x**(ax-1) * (1-x)**(bx-1) \n",
    "def beta_coef(y, x, ax, bx, ay, by):\n",
    "    return beta_n(x, ax, bx) * beta_n(y, ay, by)\n",
    "def beta(y, x):\n",
    "    return beta_coef(y, x, 10, 10, 11, 9)\n",
    "\n",
    "dblquad(beta, 0, 1, 0, lambda x: x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ps = 0.1 * np.random.rand(10, 1)\n",
    "scores = []\n",
    "num_noties = []\n",
    "for _ in range(10000):\n",
    "    match_notie = np.random.rand(*ps.shape) < ps\n",
    "    num_noties.append(match_notie.sum())\n",
    "\n",
    "    signs = np.sign(np.random.randn(*ps.shape))[match_notie > 0]\n",
    "    s = signs.sum()\n",
    "    scores.append(s)\n",
    "\n",
    "plt.hist(scores, bins=50)\n",
    "plt.figure()\n",
    "plt.hist(num_noties, bins=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py-irt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/nd-ball/py-irt/d2a27dd55a84459782a5514e752ee48d9a63626e/test_fixtures/minitest.jsonlines\n",
    "!cat minitest.jsonlines\n",
    "\n",
    "!py-irt train 1pl minitest.jsonlines test-1pl/ --lr 0.02 --epochs 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arena\n",
    "import importlib\n",
    "importlib.reload(arena)\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rng\n",
    "\n",
    "tie_probs = np.concatenate((1 - 0.05 * np.random.rand(100), 0*np.random.rand(100)))\n",
    "weights = rng.rand(tie_probs.size)\n",
    "# print(tie_probs)\n",
    "\n",
    "samps = []\n",
    "for _ in range(1000):\n",
    "    p = tie_probs.size\n",
    "    response_a = (rng.rand(p) > tie_probs) * np.sign(rng.randn(p))\n",
    "    response_b = response_a * -1\n",
    "    response_b = np.sign(rng.randn(p))\n",
    "    cdf, pvalue = arena.sign_test_niid(response_a, response_b, weights, tie_probs)\n",
    "    samps.append(pvalue)\n",
    "\n",
    "plt.hist(samps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cdf)\n",
    "ax = plt.subplot()\n",
    "cdf.plot(ax)\n",
    "print(cdf.evaluate(-0.1))\n",
    "print(cdf.evaluate(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_uppercase(s):\n",
    "    \"\"\"\n",
    "    Write a function to remove uppercase substrings from a given string.\n",
    "    assert remove_uppercase('cAstyoUrFavoRitETVshoWs') == 'cstyoravoitshos'\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    temp = ''\n",
    "    for char in s:\n",
    "        if char.isupper():\n",
    "            if temp:\n",
    "                result.append(temp)\n",
    "                temp = ''\n",
    "        else:\n",
    "            temp += char\n",
    "    if temp:\n",
    "        result.append(temp)\n",
    "    return ''.join(result)\n",
    "\n",
    "\n",
    "print(remove_uppercase('cAstyoUrFavoRitETVshoWs'))\n",
    "if '':\n",
    "   print('true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arena\n",
    "import importlib\n",
    "importlib.reload(arena)\n",
    "def trinomial(na, nb, n0):\n",
    "    n = na + nb + n0\n",
    "    cdf, pvalue = arena.sign_test_niid(([1]*na + [0]*nb + [0]*n0), np.array([0]*na + [1]*nb + [0]*n0), tie_probs=None, weights=None, sample_all=False)\n",
    "    cdf, pvalue = arena.sign_test_niid(np.array([1]*na + [0]*nb + [0]*n0), np.array([0]*na + [1]*nb + [0]*n0), tie_probs=n0 / n * np.array([1] * n), weights=None, sample_all=True)\n",
    "    print('binom', stats.binomtest(na, na + nb, p=0.5).pvalue)\n",
    "    return pvalue\n",
    "\n",
    "# trinomial(20, 12, 133)\n",
    "\n",
    "def bootstrap_consistency(battles: pd.Series, num_round=1000, interpolation='nearest'):\n",
    "    rows = []\n",
    "    counts = Counter(battles)\n",
    "    sign = np.sign(counts['model_a'] - counts['model_b'])\n",
    "    for i in range(num_round):\n",
    "        counts = Counter(battles.sample(frac=1.0, replace=True))\n",
    "        diff = counts['model_a'] - counts['model_b']\n",
    "        rows.append(diff)\n",
    "    return 1 - np.mean(np.sign(rows) == sign)\n",
    "\n",
    "\n",
    "\n",
    "print(bootstrap_ci(pd.Series(['model_a', 'model_b', 'model_a', 'both']*2)))\n",
    "    \n",
    "thres = stats.chi2.ppf(1-0.1, 1)\n",
    "print(thres, np.mean(np.random.randn(100000)**2 > thres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.binomtest(61, 100, p=0.5).pvalue)\n",
    "print(stats.binomtest(61, 100, p=0.5, alternative='greater').pvalue * 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codegen_240116_sida",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
