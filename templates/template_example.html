<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />   <!--It is necessary to use the UTF-8 encoding with plotly graphics to get e.g. negative signs to render correctly -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />

<link
  rel="stylesheet"
  href="https://crux-eval.github.io/static/css/bulma.min.css"
>
<link
  rel="stylesheet"
  href="static/custom.css"
>
</head>
<body>
<div class="container">
<h1 class="title"> {{benchmark_id}}: by examples</h1>
<h2 class="subtitle is-4"> <a href=".">Home</a> <a href="https://github.com/crux-eval/eval-arena">Doc/Code</a> </h2>

<h2 class="title is-4">Not solved by any model</h2>
<p>There are {{outputs['list_no_solve']|length}} examples not solved by any model.
   Solving some of these can be a good signal that your model is indeed better than leading models if these are good problems.  <br>
  {{outputs['list_no_solve']|join(', ')}}</p>

<h2 class="title is-4">Problems solved by 1 model only</h2>
<p>{{outputs['table_one_solve']}}</p>

<h2 class="title is-4" id="suspect">Suspect problems</h2>
<p>These are 10 problems with the lowest correlation with the overall evaluation (i.e. better models tend to do worse on these. ) </p>
<p>{{outputs['table_suspect']}}</p>

<h2 class="title is-4" id="hist">Histogram of accuracies</h2>
<p> Histogram of problems by the accuracy on each problem. </p>
<p>{{outputs['table_histogram_accs']}}</p>

<h2 class="title is-4" id="difficulty">Histogram of difficulties</h2>
<p> Histogram of problems by the minimum Elo to solve each problem. </p>
<p>{{outputs['fig_min_elo_solve']}}</p>
</div>
</body>
</html>