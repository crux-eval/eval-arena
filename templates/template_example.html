<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />   <!--It is necessary to use the UTF-8 encoding with plotly graphics to get e.g. negative signs to render correctly -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="stylesheet"
  href="https://crux-eval.github.io/static/css/bulma.min.css"
>
<style>
    table { 
      white-space: nowrap;
      text-align: right;
    }
</style>
</head>


<body>


<section class="section">
<div class="container is-max-desktop">
<div class="columns is-centered has-text-centered">
<div class="column is-full">
    <h2 class="title is-3"> {{benchmark_id}}: by examples </h2>
    <h3 class="title is-4">  <a href=".">Summary</a> &nbsp; <a href="https://github.com/crux-eval/eval-arena">Code</a> </h3>
    <div class="content has-text-justified" style="font-size: 120%;">
        <section id="nosolve">
          <h2>Not solved by any model</h2>
          <p>There are {{outputs['list_no_solve']|length}} examples not solved by any model.
              Solving some of these can be a good signal that your model is indeed better than leading models if these are good problems.  <br>
          {{outputs['list_no_solve']|join(', ')}}</p>
        </section>
  
        <h2>Problems solved by 1 model only</h2>
        <p>{{outputs['table_one_solve']}}</p>
  
        <h2 id="suspect">Suspect problems</h2>
        <p>These are 10 problems with the lowest correlation with the overall evaluation (i.e. better models tend to do worse on these. ) </p>
        <p>{{outputs['table_suspect']}}</p>

        <h2 id="hist">Histogram of accuracies</h2>
        <p> Histogram of problems by the accuracy on each problem. </p>
        <p>{{outputs['table_histogram_accs']}}</p>

        <h2 id="difficulty">Histogram of difficulties</h2>
        <p> Histogram of problems by the minimum Elo to solve each problem. </p>
        <p>{{outputs['fig_min_elo_solve']}}</p>

    </div>
</div>
</div>
</div>
</section>
</body>
</html>