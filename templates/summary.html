<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />   <!--It is necessary to use the UTF-8 encoding with plotly graphics to get e.g. negative signs to render correctly -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="stylesheet"
  href="https://crux-eval.github.io/static/css/bulma.min.css"
>
<style>
  table { 
    white-space: nowrap;
    text-align: right;
  }
</style>
</head>

<body>

<section class="section">
<div class="container is-max-desktop">
<div class="columns is-centered has-text-centered">
<div class="column is-full">
    <h2 class="title is-3"> Eval-Arena: noise and errors by leaderboard showdowns </h2>
    <h3 class="title is-4"> <a href="https://github.com/crux-eval/eval-arena">Doc/Code</a> </h3>
    <div class="content has-text-justified" style="font-size: 120%;">
    <p>
      We ran pairwise match-ups on thousands of model pairs on test-based benchmarks for code generation.
      The main results and code is <a href="https://github.com/crux-eval/eval-arena">here</a>.
      <ul>
        <li><b>size</b>: number of examples in the benchmark</li>
        <li><b>p5_min</b>: the minimum difference to achieve a p-value of 0.05</li>
        <li><b>p5_max</b>: the maximum difference not achieving a p-value of 0.05</li>
        <li><b>no_solve</b>: examples not solved by any models</li>
        <li><b>tau-</b>: examples negatively correlated with the overall model quality as measured by Kendall's tau.</li>
        <li><b>details</b>: link to details for each benchmark, aggregating by the models or by examples in each benchmark, and a plot of all models and examples sorted by difficulty. </li>
      </ul>
    </p>

    <p>{{ percent_table }}</p>

    <p>The same information but in raw counts. </p>
    <p>{{ count_table }}</p>
    <ul>
      <li><a href="https://evalplus.github.io/">EvalPlus for MBPP/+, HumanEval+</a> </li>
      <li><a href="https://github.com/openai/human-eval">HumanEval</a> </li>
      <li><a href="https://github.com/google-research/google-research/tree/master/mbpp">MBPP (we used the EvalPlus version instead of original)</a> </li>
      <li><a href="https://livecodebench.github.io/leaderboard.html">LiveCodeBench</a></li>
      <li><a href="https://crux-eval.github.io/">CRUXEval</a></li>
      <li><a href="https://ds1000-code-gen.github.io/">DS1000</a></li>
    </ul>
    
</div>
</div>
</div>
</section>


</body>
</html>