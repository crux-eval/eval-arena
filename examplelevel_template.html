<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />   <!--It is necessary to use the UTF-8 encoding with plotly graphics to get e.g. negative signs to render correctly -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="stylesheet"
  href="https://crux-eval.github.io/static/css/bulma.min.css"
>
<style>
    table { 
      white-space: nowrap;
      text-align: right;
    }
</style>
</head>


<body>


<section class="section" style="margin:0; padding:0;">
<div class="container is-max-desktop">
<div class="columns is-centered has-text-centered">
<div class="column is-full">
    <h2 class="title is-3"> {{benchmark_id}} </h2>
    <h3>  <a href=".">Summary</a> &nbsp; <a href="https://github.com/crux-eval/eval-arena">Code</a> </h3>

    <div class="content has-text-justified" style="font-size: 120%;">

        <h2>Example level results</h2>
        <p>There are {{outputs['list_no_solve']|length}} examples not solved by any model.
            Solving some of these can be a good signal that your model is indeed better than leading models if these are good problems.  <br>
        {{outputs['list_no_solve']|join(', ')}}</p>

        <h2>Problems solved by 1 model only</h2>
        <p>{{outputs['table_one_solve']}}</p>

        <h2>Histogram of accuracies</h2>
        <p>{{outputs['table_histogram_accs']}}</p>

        <h2>Mininum elo to solve each problems</h2>
        <p>For a mediocre model A, problems only solvable by much better models can be significant. </p>
        <p>{{outputs['fig_min_elo_solve']}}</p>

        <h2>Suspect problems</h2>
        <p>These are 10 problems with the lowest correlation with model ratings (i.e. better models tend to do worse on these. ) </p>
        <p>{{outputs['table_suspect']}}</p>
    </div>
</div>
</div>
</div>
</section>


</body>
</html>