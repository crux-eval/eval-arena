{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json, math, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "from report_agg import result_table, pass1_to_battle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def get_example_ratings(benchmark_id):\n",
    "    result = eval_results[eval_results['benchmark_id'] == benchmark_id]\n",
    "\n",
    "    battles = pass1_to_battle(result)\n",
    "    battles_no_ties = battles[battles[\"winner\"].str.contains(\"model_\")]\n",
    "    all_stats = result_table(battles_no_ties, result)\n",
    "    # example_table = gen_example_table(result, all_stats)\n",
    "    display(all_stats)\n",
    "    records = []\n",
    "    len_data = len(set(result['example_id']))\n",
    "    ids = set(result['example_id']) \n",
    "    len_data = len(set(result['example_id']))\n",
    "    print(np.mean(all_stats['elo']))\n",
    "    \n",
    "    for current_id in list(ids):\n",
    "        example_data = result[result['example_id'] == current_id][['model', 'pass1']]\n",
    "        fit_data = example_data.merge(all_stats[['model', 'elo']], left_on = 'model', right_on = 'model')\n",
    "        # fit_data['result'] = fit_data['result']\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        lr = LogisticRegression()\n",
    "        # display(fit_data)\n",
    "        fit_data['correct'] = np.where(fit_data['pass1'] > 0, 1, 0)\n",
    "        if all(fit_data['correct'].to_numpy() == 0) or all(fit_data['correct'].to_numpy() == 1):\n",
    "            mean = 1 if all(fit_data['correct'].to_numpy() == 1) else 0\n",
    "            records.append({\n",
    "                'sample_id': current_id,\n",
    "                'elo': -3500 if mean > 0 else 3500,\n",
    "                'elo_var': 0,\n",
    "                'score': 1,\n",
    "                'acc': mean,\n",
    "            })\n",
    "        else:\n",
    "            Xd = fit_data['elo'].to_numpy().reshape(-1, 1)\n",
    "            yd = fit_data['correct'].to_numpy() \n",
    "            lrm = lr.fit(fit_data['elo'].to_numpy().reshape(-1, 1), fit_data['correct'].to_numpy())\n",
    "            X = list(range(500, 1500, 10))\n",
    "            y = [lrm.predict_proba([[x]])[0, 1] for x in X]\n",
    "            elo = -lrm.intercept_[0] / lrm.coef_[0][0]\n",
    "            elo_var = 1/lrm.coef_[0][0]\n",
    "            score = lrm.score(Xd, yd)\n",
    "            records.append({\n",
    "                'sample_id': current_id,\n",
    "                'elo': elo,\n",
    "                'elo_var': elo_var,\n",
    "                'score': score,\n",
    "                'acc': fit_data['correct'].to_numpy().mean()\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "records = []\n",
    "for fname in glob.glob(f\"data/*.jsonl\"):\n",
    "    with open(fname, 'rt') as f:\n",
    "        records.extend([json.loads(l) for l in f.readlines()])\n",
    "\n",
    "eval_results = pd.DataFrame(records)\n",
    "for b in ['humaneval+']:\n",
    "    df = get_example_ratings(b)\n",
    "    display(df)\n",
    "\n",
    "from report_example import gen_example_table\n",
    "\n",
    "def get_example_level_results(benchmark_id):\n",
    "    result = eval_results[eval_results['benchmark_id'] == benchmark_id]\n",
    "    battles = pass1_to_battle(result)\n",
    "    battles_no_ties = battles[battles[\"winner\"].str.contains(\"model_\")]\n",
    "    all_stats = result_table(battles_no_ties, result)\n",
    "    example_table = gen_example_table(result, all_stats)\n",
    "    return example_table\n",
    "\n",
    "table = get_example_level_results('CRUXEval-input')\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(table['tau'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for fname in glob.glob(f\"data/*.jsonl\"):\n",
    "    with open(fname, 'rt') as f:\n",
    "        records.extend([json.loads(l) for l in f.readlines()])\n",
    "\n",
    "eval_results = pd.DataFrame(records)[['benchmark_id', 'model', 'example_id', 'pass1']]\n",
    "# display(eval_results)\n",
    "\n",
    "def get_result(benchmark_id):\n",
    "    return eval_results[eval_results['benchmark_id'] == benchmark_id]\n",
    "\n",
    "def format_irt(result):\n",
    "    \"\"\"\n",
    "    {\"subject_id\": \"pedro\",    \"responses\": {\"q1\": 1, \"q2\": 0, \"q3\": 1, \"q4\": 0}}\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for name, g in result[['example_id', 'model', 'pass1']].groupby('model'):\n",
    "        records.append(\n",
    "            {'subject_id': name, 'responses': {r.example_id: r.pass1 for r in g.itertuples()}}\n",
    "        )\n",
    "    print(records)\n",
    "    return records\n",
    "\n",
    "dfb = get_result('humaneval+')\n",
    "display(dfb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def format_irt(results):\n",
    "    \"\"\"\n",
    "    output {\"subject_id\": \"pedro\", \"q1\": 1, \"q2\": 0, \"q3\": 1, \"q4\": 0}\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for name, g in results[['example_id', 'model', 'pass1']].groupby('model'):\n",
    "        records.append(\n",
    "            {'subject_id': name, **{r.example_id: r.pass1 for r in g.itertuples()}}\n",
    "        )\n",
    "    return records\n",
    "\n",
    "def irt_ability(results):\n",
    "    from py_irt.config import IrtConfig\n",
    "    from py_irt.training import IrtModelTrainer\n",
    "    from py_irt.dataset import Dataset\n",
    "    from py_irt.models import OneParamLog\n",
    "    df_irt = pd.DataFrame(format_irt(results))\n",
    "    dataset = Dataset.from_pandas(df_irt, subject_column='subject_id')\n",
    "    config = IrtConfig(model_type='2pl', log_every=200, dropout=0.2)\n",
    "    trainer = IrtModelTrainer(config=config, data_path=None, dataset=dataset)\n",
    "    trainer.train(epochs=5000, device='cpu')\n",
    "    # trainer = OneParamLog.train(dataset)\n",
    "\n",
    "    subjs = trainer.last_params['subject_ids']\n",
    "    # print(trainer.last_params.keys())\n",
    "    # dict_keys(['ability', 'diff', 'irt_model', 'item_ids', 'subject_ids'])\n",
    "    items = trainer.last_params['item_ids']\n",
    "\n",
    "    df_models = pd.DataFrame({'model': [subjs[k] for k in subjs], 'ability': trainer.last_params['ability']})\n",
    "    df_items = pd.DataFrame({'example_id': [items[k] for k in items], 'diff': trainer.last_params['diff'], 'disc': trainer.last_params['disc']})\n",
    "    return df_models, df_items\n",
    "\n",
    "def get_ratings(results):\n",
    "    battles = pass1_to_battle(results)\n",
    "    battles_no_ties = battles[battles[\"winner\"].str.contains(\"model_\")]\n",
    "    all_stats = result_table(battles_no_ties, results)\n",
    "    ability, _ = irt_ability(results)\n",
    "    all_stats = all_stats.merge(ability, on='model')\n",
    "    return all_stats\n",
    "\n",
    "def subsample(results, n=100):\n",
    "    eids = set(results['example_id'])\n",
    "    include_ids = np.random.choice(list(eids), n, replace=False)\n",
    "    return results[results['example_id'].isin(include_ids)]\n",
    "\n",
    "\n",
    "def compare_subsamples(benchmark_id):\n",
    "    result = eval_results[eval_results['benchmark_id'] == benchmark_id]\n",
    "    result1 = subsample(result)\n",
    "    result2 = subsample(result)\n",
    "    stats1 = get_ratings(result1)\n",
    "    stats2 = get_ratings(result2)\n",
    "    both_stats = stats1.merge(stats2, on='model')\n",
    "    display(both_stats)\n",
    "    # fig = px.scatter(both_stats, 'pass1_x', 'pass1_y')\n",
    "    # fig = px.scatter(both_stats, 'pass1_x', 'pass1_y')\n",
    "    # display(fig)\n",
    "    return both_stats\n",
    "\n",
    "for b in set(eval_results['benchmark_id']):\n",
    "    result = eval_results[eval_results['benchmark_id'] == b]\n",
    "    result['pass1'] = np.where(result['pass1'] > 0.1, 1, 0)\n",
    "    df_irt = pd.DataFrame(format_irt(result))\n",
    "    display(df_irt)\n",
    "    with open(f'irt_data/{b}.jsonl', 'w') as f:\n",
    "        f.write(df_irt.to_json(orient='records', lines=True, index=False))\n",
    "# df_irt.save()\n",
    "\n",
    "# result['pass1'] = np.where(result['pass1'] > 0.1, 1, 0)\n",
    "# abilities, items = irt_ability(result)\n",
    "# display(items)\n",
    "\n",
    "# sns.scatterplot(items, x='diff', y='disc')\n",
    "\n",
    "# display(all_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_stats = compare_subsamples('mbpp+')\n",
    "display(both_stats)\n",
    "plt.figure()\n",
    "sns.scatterplot(both_stats, x='elo_x', y='elo_y')\n",
    "plt.figure()\n",
    "sns.scatterplot(both_stats, x='pass1_x', y='pass1_y')\n",
    "plt.figure()\n",
    "sns.scatterplot(both_stats, x='ability_x', y='ability_y')\n",
    "\n",
    "# for prefix in ['ability', 'pass1', 'win_rate', 'elo']:\n",
    "#     for prefix2 in ['ability', 'pass1', 'win_rate', 'elo']:\n",
    "#         tau = stats.kendalltau(both_stats[f'{prefix}_x'], both_stats[f'{prefix2}_x'])\n",
    "#         print(prefix, prefix2, f'{tau.statistic:.4f}')\n",
    "\n",
    "for prefix in ['ability', 'pass1', 'win_rate', 'elo']:\n",
    "    tau = stats.kendalltau(both_stats[f'{prefix}_x'], both_stats[f'{prefix}_y'])\n",
    "    print(prefix, f'{tau.statistic:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(items, y='disc', x='diff')\n",
    "df_irt = pd.DataFrame(format_irt(result))\n",
    "battles = pass1_to_battle(result)\n",
    "battles_no_ties = battles[battles[\"winner\"].str.contains(\"model_\")]\n",
    "all_stats = result_table(battles_no_ties, result)\n",
    "# example_table = gen_example_table(result, all_stats)\n",
    "display(all_stats)\n",
    "display(abilities)\n",
    "ids = list(set(result['example_id']))\n",
    "for id in ids[100:120]:\n",
    "    plt.figure()\n",
    "    result_id = result[result['example_id'] == id]\n",
    "    pred_v_ability = result_id.merge(all_stats[['model', 'elo']], on = 'model')\n",
    "    pred_v_ability = pred_v_ability.merge(abilities[['model', 'ability']], on = 'model')\n",
    "    # display(pred_v_ability)\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.scatterplot(pred_v_ability, x='elo', y='pass1')\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.scatterplot(pred_v_ability, x='ability', y='pass1')\n",
    "    item_info = items[items['example_id'] == id]\n",
    "    plt.title(item_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import report_agg\n",
    "from report_agg import result_table, pass1_to_battle\n",
    "import numpy.random as rng\n",
    "import arena\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "importlib.reload(arena)\n",
    "\n",
    "result = eval_results[eval_results['benchmark_id'] == 'mbpp+']\n",
    "battles = pass1_to_battle(result)\n",
    "\n",
    "def estimate_tie_probs(battles: pd.DataFrame):\n",
    "    pass\n",
    "\n",
    "estimate_tie_probs(battles)\n",
    "win_probs = battles.groupby(by='example_id')[['winner']].aggregate(lambda x: 2*np.mean(x == 'model_a'))\n",
    "tie_probs = battles.groupby(by='example_id')[['winner']].aggregate(lambda x: np.mean((x == 'neither') | (x == 'both')))\n",
    "tie_probs = battles.groupby(by='example_id')[['winner']].aggregate(lambda x: np.mean((x == 'neither')))\n",
    "# display(tie_probs.describe())\n",
    "# tie_probs.hist()\n",
    "display(tie_probs)\n",
    "\n",
    "m = list(set(battles['model_a']))\n",
    "display(battles.head())\n",
    "# ma = 'claude-3-opus-20240229'\n",
    "# mb = 'opencodeinterpreter-ds-33b'\n",
    "# mb = 'meta-llama-3-70b-instruct'\n",
    "# ma = 'deepseek-coder-33b-instruct'\n",
    "# mb = 'deepseek-coder-6.7b-instruct'\n",
    "ma = 'codellama-34b'\n",
    "mb = 'codellama-13b'\n",
    "# mb = 'codellama-7b'\n",
    "ma = 'gpt-4-1106-preview'\n",
    "mb = 'meta-llama-3-70b-instruct'\n",
    "result_a = result[result['model'] == ma][['example_id', 'pass1']]\n",
    "result_b = result[result['model'] == mb][['example_id', 'pass1']]\n",
    "result_ab = result_a.merge(result_b, on='example_id', suffixes=['_a', '_b'])\n",
    "res_withprob = pd.merge(tie_probs, result_ab, on='example_id')\n",
    "display(res_withprob)\n",
    "res_withprob['winner'].hist()\n",
    "tie_probs = res_withprob['winner'].to_numpy()\n",
    "win_prob = (1 - tie_probs) \n",
    "weights = 1 - win_prob \n",
    "weights = {\n",
    "    'uniform': np.ones(win_prob.shape), \n",
    "    'win_prob': win_prob,\n",
    "    'if2': np.where((0.05 < win_prob) & (win_prob < 0.8), 1.5, 1),\n",
    "    'disc': 0.5 + result_ab[['example_id']].merge(items, on='example_id')['disc'].to_numpy(),\n",
    "    'rand': np.random.rand(*win_prob.shape),\n",
    "}\n",
    "\n",
    "plt.figure()\n",
    "for w in weights:\n",
    "    we = weights[w]\n",
    "    assert all(we >= 0)\n",
    "    # cdf, pv = arena.sign_test_niid(res_withprob['pass1_a'].to_numpy(), res_withprob['pass1_b'].to_numpy(), res_withprob['winner'].to_numpy(), we) \n",
    "    cdf, pv = arena.sign_test_niid(res_withprob['pass1_a'].to_numpy(), res_withprob['pass1_b'].to_numpy(), None, we, sample_all=False) \n",
    "    plt.figure()\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    cdf.plot(ax)\n",
    "    plt.title(f'{w}\\t {pv:.3f}')\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    ax.hist(we)\n",
    "    print(w, pv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(arena)\n",
    "\n",
    "res_a = res_withprob['pass1_a'].to_numpy()\n",
    "res_b = res_withprob['pass1_b'].to_numpy()\n",
    "tie_probs = np.mean(res_a == res_b) * np.ones(res_b.size)\n",
    "# print(tie_probs)\n",
    "weights = 1*np.ones(res_b.size)\n",
    "cdf, pv = arena.sign_test_niid(res_a, res_b, tie_probs, weights, sample_all=True) \n",
    "\n",
    "print(pv)\n",
    "\n",
    "import scipy.stats as stats\n",
    "k = np.sum(res_a > res_b)\n",
    "n = np.sum(res_a != res_b)\n",
    "print(k, n)\n",
    "print('binom', stats.binomtest(k, n, p=0.5, alternative='two-sided').pvalue)\n",
    "\n",
    "ax = plt.subplot()\n",
    "cdf.plot(ax)\n",
    "\n",
    "cdf.evaluate(-0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codegen_240116_sida",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
