{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json, math, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_cruxeval(type):\n",
    "    records = []\n",
    "    for fname in glob.glob(f\"evaluation_results/*_temp0.2_{type}.json\"):\n",
    "        name = fname.split('/')[1]\n",
    "        model, temp, typejsonl = name.split('_')\n",
    "        print(model, temp, type)\n",
    "\n",
    "        with open(fname) as f:\n",
    "            res = json.load(f)['raw_scored_generations']\n",
    "            for exid in res:\n",
    "                gotid = res[exid][0]\n",
    "                records.append({\n",
    "                    'benchmark': f'cruxeval_{type}',\n",
    "                    'model': model,\n",
    "                    'example': f'{exid}_{type}',\n",
    "                    'result': gotid,\n",
    "                    'hyperparams': temp\n",
    "                })\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    display(df.describe())\n",
    "    return df\n",
    "        \n",
    "\n",
    "with open('cruxeval_input.jsonl', 'w') as f:\n",
    "    dfi = get_cruxeval('input')\n",
    "    f.write(dfi.to_json(orient='records', lines=True))\n",
    "\n",
    "with open('cruxeval_output.jsonl', 'w') as f:\n",
    "    dfo = get_cruxeval('output')\n",
    "    f.write(dfo.to_json(orient='records', lines=True))\n",
    "\n",
    "result = pd.concat([dfi, dfo])\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_to_battle(result: pd.DataFrame):\n",
    "    pa = pd.merge(result, result, on=['example'], suffixes=[\"_a\", \"_b\"], how='outer')\n",
    "\n",
    "    awins = (pa['result_a'] == True) & (pa['result_b'] == False)\n",
    "    bwins = (pa['result_a'] == False) & (pa['result_b'] == True)\n",
    "    ties_neither = (pa['result_a'] == False) & (pa['result_b'] == False)\n",
    "    ties_both = (pa['result_a'] == True) & (pa['result_b'] == True)\n",
    "    # pa[['winner']][awins] = 'model_a' \n",
    "    pa['winner'] = 'a'\n",
    "    pa.loc[awins, 'winner'] = 'model_a'\n",
    "    pa.loc[bwins, 'winner'] = 'model_b'\n",
    "    pa.loc[ties_neither, 'winner'] = 'neither'\n",
    "    pa.loc[ties_both, 'winner'] = 'both'\n",
    "    return pa \n",
    "\n",
    "battles = result_to_battle(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def visualize_battle_count(battles, title, show_num_models=30):\n",
    "    ptbl = pd.pivot_table(battles, index=\"model_a\", columns=\"model_b\", aggfunc=\"size\",\n",
    "                          fill_value=0)\n",
    "    battle_counts = ptbl\n",
    "    ordering = battle_counts.sum().sort_values(ascending=True).index\n",
    "    ordering = ordering[:show_num_models]\n",
    "    fig = px.imshow(battle_counts.loc[ordering, ordering],\n",
    "                    title=title, text_auto=True)\n",
    "    fig.update_layout(xaxis_title=\"Model B\",\n",
    "                      yaxis_title=\"Model A\",\n",
    "                      xaxis_side=\"top\", height=800, width=800,\n",
    "                      title_y=0.07, title_x=0.5,\n",
    "                      font=dict(size=10))\n",
    "    fig.update_traces(hovertemplate=\n",
    "                      \"Model A: %{y}<br>Model B: %{x}<br>Count: %{z}<extra></extra>\")\n",
    "    return fig\n",
    "\n",
    "battles_no_ties = battles[battles[\"winner\"].str.contains(\"model_\")]\n",
    "# fig = visualize_battle_count(battles_no_ties, \"No ties\")\n",
    "# # fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_pairwise_win_fraction(battles, max_num_models=30):\n",
    "    # Times each model wins as Model A\n",
    "    a_win_ptbl = pd.pivot_table(\n",
    "        battles[battles['winner'] == \"model_a\"],\n",
    "        index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0)\n",
    "\n",
    "    # Table counting number of A-B pairs\n",
    "    num_battles_ptbl = pd.pivot_table(battles,\n",
    "        index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0)\n",
    "\n",
    "    # Computing the proportion of wins for each model as A and as B\n",
    "    # against all other models\n",
    "    row_beats_col_freq = (\n",
    "        (a_win_ptbl) /\n",
    "        (num_battles_ptbl)\n",
    "    )\n",
    "    # display(mcnemar)\n",
    "    # Arrange ordering according to proprition of wins\n",
    "    prop_wins = row_beats_col_freq.mean(axis=1).sort_values(ascending=False)\n",
    "    prop_wins = prop_wins[:max_num_models]\n",
    "    model_names = list(prop_wins.keys())\n",
    "    row_beats_col = row_beats_col_freq.loc[model_names, model_names]\n",
    "\n",
    "    wins = a_win_ptbl.loc[model_names, model_names]\n",
    "    diffs = (wins - wins.T)\n",
    "    sums = (wins + wins.T)\n",
    "    mcnemar = diffs ** 2 / sums\n",
    "    mcnemar = mcnemar.apply(lambda x: 1 - stats.chi2.cdf(x, 1))\n",
    "\n",
    "    return row_beats_col, mcnemar, diffs, sums \n",
    "\n",
    "def visualize_pairwise_win_fraction(battles, title, max_num_models=30):\n",
    "    row_beats_col, mcnemar, diffs, sums = compute_pairwise_win_fraction(battles, max_num_models)\n",
    "    fig = px.imshow(mcnemar,\n",
    "                    text_auto=\".2f\", title=title)\n",
    "    fig.update_layout(xaxis_title=\" Model B: Loser\",\n",
    "                  yaxis_title=\"Model A: Winner\",\n",
    "                  xaxis_side=\"top\", height=900, width=900,\n",
    "                  title_y=0.07, title_x=0.5)\n",
    "\n",
    "    sort_keys = row_beats_col.keys() \n",
    "    extra_info = (pd.concat([row_beats_col, mcnemar, diffs, sums])\n",
    "    .stack()\n",
    "    .groupby(level=[0,1])\n",
    "    .apply(tuple)\n",
    "    .unstack()\n",
    "    ).loc[sort_keys, sort_keys]\n",
    "    \n",
    "    # display(extra_info)\n",
    "    fig.update_traces(customdata=extra_info, hovertemplate=\n",
    "        \"Model A: %{y}<br>Model B: %{x}<br>p-value: %{customdata[1]}<br> A beats B rate: %{customdata[0]} <br>diffs %{customdata[2]} <br>sums %{customdata[3]}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = visualize_pairwise_win_fraction(battles_no_ties, 'test', max_num_models=60)\n",
    "fig\n",
    "# fig.to_html('mcnemar_test.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_beats_col, mcnemar, diffs, sums = compute_pairwise_win_fraction(battles, max_num_models=65)\n",
    "# display(diffs)\n",
    "\n",
    "df = diffs.reset_index().melt(id_vars='model_a').merge(sums.reset_index().melt(id_vars='model_a'), on=['model_a', 'model_b'])\n",
    "df = df[df['value_x'] > 0]\n",
    "fig = px.scatter(df[df['model_a'] != df['model_b']], x='value_x', y='value_y', custom_data=['model_a', 'model_b'])\n",
    "display(df)\n",
    "fig.update_traces(hovertemplate=\n",
    "        \"dModel A: %{customdata[0]}<br>Model B: %{customdata[1]}<br>\")\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codegen_240116_sida",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
